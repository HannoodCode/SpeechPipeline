
# AI Speech Pipeline

A modular, full-stack application that processes speech through a customizable AI pipeline: **Speech-to-Text â†’ Language Model â†’ Text-to-Speech**. Built with FastAPI backend and React frontend.

## Features

- **Speech Input**: Record audio directly in the browser
- **Text Input**: Type messages for text-only processing
- **Modular Architecture**: Choose from multiple providers for each step
- **Real-time Processing**: Stream audio through the complete pipeline
- **Service Status**: Monitor which providers are available
- **Audio Download**: Save AI responses as MP3 files
- **Modern UI**: Responsive design with Material-UI
- **Theme Support**: Light/dark mode toggle with smooth transitions
- **Side-by-Side Layout**: Compare input and output simultaneously
- **Responsive Design**: Optimized for desktop and mobile devices

## Architecture

### Speech-to-Text (STT) Providers
- **OpenAI Whisper** - Industry-leading accuracy
- **Google Speech-to-Text** - Enterprise-grade cloud API
- **Azure Speech Services** - Microsoft's speech recognition

### Language Model (LLM) Providers
- **OpenAI GPT** - GPT-3.5 Turbo, GPT-4, GPT-4 Turbo
- **Anthropic Claude** - Claude 3 (Haiku, Sonnet, Opus)
- **Ollama** - Local models (Llama 2, Mistral, CodeLlama)

### Text-to-Speech (TTS) Providers
- **Google Cloud TTS** - High-quality neural voices
- **ElevenLabs** - Premium AI voices with emotion
- **Microsoft Edge TTS** - Free neural voices
- **Google Translate TTS** - Free basic voice synthesis

## ğŸ“ Project Structure

```
speech-ai-pipeline/
â”œâ”€â”€ backend/                 # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py         # FastAPI application entry point
â”‚   â”‚   â”œâ”€â”€ api/            # API route handlers
â”‚   â”‚   â”‚   â”œâ”€â”€ stt.py      # Speech-to-text endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ llm.py      # Language model endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ tts.py      # Text-to-speech endpoints
â”‚   â”‚   â”‚   â””â”€â”€ pipeline.py # Full pipeline endpoints
â”‚   â”‚   â””â”€â”€ services/       # Modular service implementations
â”‚   â”‚       â”œâ”€â”€ stt/        # STT provider implementations
â”‚   â”‚       â”œâ”€â”€ llm/        # LLM provider implementations
â”‚   â”‚       â””â”€â”€ tts/        # TTS provider implementations
â”‚   â”œâ”€â”€ requirements.txt    # Python dependencies
â”‚   â””â”€â”€ env.example        # Environment variables template
â”œâ”€â”€ frontend/               # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/     # UI components
â”‚   â”‚   â”‚   â”œâ”€â”€ MicrophoneInput.tsx # Voice recording component
â”‚   â”‚   â”‚   â”œâ”€â”€ TextInput.tsx      # Text input component
â”‚   â”‚   â”‚   â”œâ”€â”€ AudioOutput.tsx    # Audio playback component
â”‚   â”‚   â”‚   â”œâ”€â”€ ModelSelector.tsx  # Provider selection component
â”‚   â”‚   â”‚   â”œâ”€â”€ StatusIndicator.tsx # Service status component
â”‚   â”‚   â”‚   â””â”€â”€ Footer.tsx         # Footer component
â”‚   â”‚   â”œâ”€â”€ pages/         # Page components
â”‚   â”‚   â”‚   â”œâ”€â”€ Landing.tsx        # Homepage with centered content
â”‚   â”‚   â”‚   â”œâ”€â”€ Dashboard.tsx      # Main demo page with side-by-side layout
â”‚   â”‚   â”‚   â””â”€â”€ About.tsx          # About page
â”‚   â”‚   â”œâ”€â”€ theme/         # Theme configuration
â”‚   â”‚   â”‚   â””â”€â”€ ColorModeProvider.tsx # Light/dark mode provider
â”‚   â”‚   â”œâ”€â”€ api.ts         # Backend API client
â”‚   â”‚   â”œâ”€â”€ types.ts       # TypeScript type definitions
â”‚   â”‚   â””â”€â”€ App.tsx        # Main application component with Material-UI navigation
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â””â”€â”€ logo.png       # Application logo
â”‚   â””â”€â”€ package.json       # Node.js dependencies
â””â”€â”€ README.md              # This file
```

## Quick Start

### Prerequisites

- **Python 3.8+**
- **Node.js 16+**
- **API Keys** for your chosen providers (see Configuration below)

### 1. Clone Repository

```bash
git clone <your-repo-url>
cd speech-ai-pipeline
```

### 2. Backend Setup

```bash
cd backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure environment variables
cp env.example .env
# Edit .env with your API keys (see Configuration section)

# Start the backend server
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

The backend will be available at `http://localhost:8000`

### 3. Frontend Setup

```bash
cd frontend

# Install dependencies
npm install

# Start the development server
npm start
```

The frontend will be available at `http://localhost:3000`

## Configuration

### Environment Variables

Copy `backend/env.example` to `backend/.env` and configure your API keys:

```bash
# OpenAI (for Whisper STT and GPT models)
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic (for Claude models)
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Google Cloud (for Speech-to-Text and Text-to-Speech)
GOOGLE_APPLICATION_CREDENTIALS=path/to/your/google-credentials.json

# Azure Speech Services
AZURE_SPEECH_KEY=your_azure_speech_key_here
AZURE_SPEECH_REGION=eastus

# ElevenLabs (for premium TTS)
ELEVENLABS_API_KEY=your_elevenlabs_api_key_here

# Ollama (local LLM server)
OLLAMA_BASE_URL=http://localhost:11434
```

### Provider Setup Guides

#### OpenAI Setup
1. Get API key from [OpenAI Platform](https://platform.openai.com/api-keys)
2. Add to `.env`: `OPENAI_API_KEY=sk-...`

#### Google Cloud Setup
1. Create project at [Google Cloud Console](https://console.cloud.google.com/)
2. Enable Speech-to-Text and Text-to-Speech APIs
3. Create service account and download JSON key
4. Set `GOOGLE_APPLICATION_CREDENTIALS` to JSON file path

#### Azure Speech Setup
1. Create Speech resource at [Azure Portal](https://portal.azure.com/)
2. Get key and region from resource overview
3. Add to `.env`: `AZURE_SPEECH_KEY` and `AZURE_SPEECH_REGION`

#### Anthropic Setup
1. Get API key from [Anthropic Console](https://console.anthropic.com/)
2. Add to `.env`: `ANTHROPIC_API_KEY=sk-ant-...`

#### ElevenLabs Setup
1. Sign up at [ElevenLabs](https://elevenlabs.io/)
2. Get API key from profile
3. Add to `.env`: `ELEVENLABS_API_KEY=...`

#### Ollama Setup (Local LLMs)
1. Install [Ollama](https://ollama.ai/)
2. Pull models: `ollama pull llama2`, `ollama pull mistral`
3. Start server: `ollama serve`

## ğŸ”§ API Endpoints

### Core Pipeline
- `POST /api/pipeline/process` - Full pipeline (Audio â†’ STT â†’ LLM â†’ TTS â†’ Audio)
- `POST /api/pipeline/process-text` - Text pipeline (Text â†’ LLM â†’ TTS â†’ Audio)
- `GET /api/pipeline/status` - Check service availability

### Individual Services
- `POST /api/stt/transcribe` - Speech-to-text only
- `POST /api/llm/generate` - Language model only
- `POST /api/tts/synthesize` - Text-to-speech only

### Provider Information
- `GET /api/providers` - List all available providers
- `GET /api/stt/providers` - STT provider details
- `GET /api/llm/providers` - LLM provider details
- `GET /api/tts/providers` - TTS provider details

## Usage Examples

### Basic Voice Interaction
1. Navigate to the Demo page using the top navigation
2. Select your preferred providers (e.g., Whisper + OpenAI + ElevenLabs)
3. Click "Start Recording" and speak your message in the Voice Input card
4. Click "Stop Recording" then "Process Audio"
5. Listen to the AI's response in the AI Response card (right side)

### Text Chat
1. Choose LLM and TTS providers from the configuration panel
2. Type your message in the Text Input card (center)
3. Click "Process Text" or use Ctrl+Enter
4. Hear the AI's spoken response in the AI Response card (right side)

### UI Features
- **Side-by-Side Layout**: Voice Input (left), Text Input (center), AI Response (right)
- **Theme Toggle**: Switch between light and dark modes using the toggle in the top-right
- **Responsive Design**: Cards stack vertically on mobile, side-by-side on desktop
- **Material-UI Navigation**: Clean top navigation bar with Home, Demo, and About links
- **Real-time Status**: Monitor service availability and processing status

### Custom Configuration
- Change providers mid-conversation
- Adjust language settings for multilingual support
- Select different voices for varied experiences
- Monitor service status in real-time

## Deployment

### Docker Deployment (Coming Soon)
```bash
# Build and run with Docker Compose
docker-compose up --build
```

### Manual Deployment
1. **Backend**: Deploy FastAPI app with Gunicorn/Uvicorn
2. **Frontend**: Build with `npm run build` and serve static files
3. **Environment**: Set production API keys and endpoints

## Development

### Adding New Providers

1. **STT Provider**:
   - Create service class in `backend/app/services/stt/`
   - Implement `transcribe()` and `is_available()` methods
   - Register in `backend/app/api/stt.py`

2. **LLM Provider**:
   - Create service class in `backend/app/services/llm/`
   - Implement `generate()` and `chat()` methods
   - Register in `backend/app/api/llm.py`

3. **TTS Provider**:
   - Create service class in `backend/app/services/tts/`
   - Implement `synthesize()` and `get_voices()` methods
   - Register in `backend/app/api/tts.py`

### Frontend Development
```bash
cd frontend
npm start    # Development server
npm test     # Run tests
npm run build # Production build
```

### Backend Development
```bash
cd backend
uvicorn app.main:app --reload  # Development server with auto-reload
pytest                         # Run tests (when implemented)
```

## ğŸ“‹ Roadmap

- [x] Modern UI with Material-UI navigation
- [x] Light/dark theme support with smooth transitions
- [x] Responsive design for mobile and desktop
- [ ] Docker containerization
- [ ] Conversation history
- [ ] Voice cloning integration
- [ ] Real-time streaming pipeline
- [ ] Webhook integrations
- [ ] Mobile app (React Native)
- [ ] Voice activity detection
- [ ] Multi-language auto-detection

## Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- **OpenAI** for Whisper and GPT models
- **Anthropic** for Claude models
- **Google Cloud** for Speech APIs
- **Microsoft** for Azure Speech Services
- **ElevenLabs** for premium voice synthesis
- **Material-UI** for React components and theming
- **FastAPI** for the excellent Python web framework
- **React Router** for client-side routing

## ğŸ”— Links

- **Demo**: [Live Demo](your-demo-url)
- **Documentation**: [API Docs](http://localhost:8000/docs)
- **Portfolio**: [Your Portfolio](https://github.com/HannoodCode)